{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¨ Welcome to the Image Colorization Project\n",
    "\n",
    "## Overview\n",
    "Welcome ðŸ§‘â€ðŸ”¬ðŸ‘©â€ðŸ’», to our thrilling exploration into the realm of image colorization! In this project, we'll be immersing ourselves in the captivating task of adding color to grayscale images ðŸ–¼ï¸. Our objective is to develop a model that can accurately predict and apply colors to grayscale images, revitalizing them with vibrant hues ðŸŒˆ.\n",
    "\n",
    "## Project Objectives\n",
    "- **Understanding Image Colorization**: Gain insights into the principles and techniques behind image colorization ðŸŽ¨.\n",
    "- **Exploring Deep Learning Models**: Dive into the world of deep learning architectures like U-Net for image colorization ðŸ¤–.\n",
    "- **Model Training and Optimization**: Learn how to train and fine-tune neural networks to achieve optimal colorization results ðŸ§ .\n",
    "- **Real-World Applications**: Apply your skills to create a model capable of efficiently colorizing grayscale images, opening avenues for various practical applications ðŸŒŸ.\n",
    "\n",
    "## Dataset\n",
    "We'll be utilizing the COCO (Common Objects in Context) dataset, which provides a diverse collection of grayscale images paired with their corresponding color images ðŸ“¸. This dataset will serve as the foundation for training our U-Net-like network to accurately predict colors for grayscale inputs.\n",
    "\n",
    "We compiled this dataset from the COCO dataset, ensuring diversity and quality.\n",
    "If you're excited about this project, drop a message below! ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš ï¸ **Important: Use GPU Runtime** âš ï¸\n",
    "\n",
    "To ensure this notebook functions correctly and efficiently, it is **crucial to use a GPU runtime**. Follow these steps to enable GPU acceleration:\n",
    "\n",
    "1. **Open Runtime settings**: At the top of the page, click on `Runtime` in the menu bar. ðŸ”„\n",
    "\n",
    "2. **Change the runtime type**: In the dropdown menu, select `Change runtime type`. ðŸ› ï¸\n",
    "\n",
    "3. **Select GPU as the hardware accelerator**: In the dialog that appears, under `Hardware accelerator`, choose `GPU T4` from the dropdown menu. ðŸ–¥ï¸\n",
    "\n",
    "4. **Save the settings**: Click `Save` to apply the changes. ðŸ’¾\n",
    "\n",
    "By enabling GPU, the computations in this notebook will be significantly faster, especially for tasks like training neural networks, processing large datasets, or performing complex calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "\n",
    "import torch\n",
    "from utils import * \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_colab = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastai==2.4 &> /dev/null\n",
    "!pip install ipywidgets widgetsnbextension pandas-profiling &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.external import untar_data, URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_path = untar_data(URLs.COCO_SAMPLE)\n",
    "coco_path = str(coco_path) + \"/train_sample\"\n",
    "use_colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_colab == True:\n",
    "    path = coco_path\n",
    "else:\n",
    "    path = \"Your path to the dataset\"\n",
    "\n",
    "paths = glob.glob(path + \"/*.jpg\") # Grabbing all the image file names\n",
    "np.random.seed(123)\n",
    "paths_subset = np.random.choice(paths, 10_000, replace=False) # choosing 1000 images randomly\n",
    "rand_idxs = np.random.permutation(10_000)\n",
    "train_idxs = rand_idxs[:8000] # choosing the first 8000 as training set\n",
    "val_idxs = rand_idxs[8000:] # choosing last 2000 as validation set\n",
    "train_paths = paths_subset[train_idxs]\n",
    "val_paths = paths_subset[val_idxs]\n",
    "print(len(train_paths), len(val_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "for ax, img_path in zip(axes.flatten(), train_paths):\n",
    "    ax.imshow(Image.open(img_path))\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = make_dataloaders(paths=train_paths, split='train')\n",
    "val_dl = make_dataloaders(paths=val_paths, split='val')\n",
    "\n",
    "data = next(iter(train_dl))\n",
    "Ls, abs_ = data['L'], data['ab']\n",
    "print(Ls.shape, abs_.shape)\n",
    "print(len(train_dl), len(val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PatchDiscriminator(3))\n",
    "# TODO: describe the Patch Discriminator role "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = PatchDiscriminator(3)\n",
    "dummy_input = torch.randn(16, 3, 256, 256) # batch_size, channels, size, size\n",
    "out = discriminator(dummy_input)\n",
    "out.shape\n",
    "\n",
    "# Analyze the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fix loss in GANLoss\n",
    "from src.ganloss import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test the weight initilizers\n",
    "from src.weight_initializer import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete this model\n",
    "class MainModel(nn.Module):\n",
    "    def __init__(self, net_G=None, lr_G=2e-4, lr_D=2e-4,\n",
    "                 beta1=0.5, beta2=0.999, lambda_L1=100.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.lambda_L1 = lambda_L1\n",
    "\n",
    "        if net_G is None:\n",
    "            self.net_G = init_model(Unet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n",
    "        else:\n",
    "            self.net_G = net_G.to(self.device)\n",
    "        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n",
    "        self.GANcriterion =\n",
    "        self.L1criterion =\n",
    "        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2))\n",
    "        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2))\n",
    "\n",
    "    def set_requires_grad(self, model, requires_grad=True):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = requires_grad\n",
    "\n",
    "    def setup_input(self, data):\n",
    "        self.L = data['L'].to(self.device)\n",
    "        self.ab = data['ab'].to(self.device)\n",
    "\n",
    "    def forward(self):\n",
    "        self.fake_color = self.net_G(self.L)\n",
    "\n",
    "    def backward_D(self):\n",
    "        # TODO: define the backward of the descriminator\n",
    "\n",
    "    def backward_G(self):\n",
    "        # TODO: define the backward of the generator \n",
    "\n",
    "    def optimize(self):\n",
    "        self.forward()\n",
    "        self.net_D.train()\n",
    "        self.set_requires_grad(self.net_D, True)\n",
    "        self.opt_D.zero_grad()\n",
    "        self.backward_D()\n",
    "        self.opt_D.step()\n",
    "\n",
    "        self.net_G.train()\n",
    "        self.set_requires_grad(self.net_D, False)\n",
    "        self.opt_G.zero_grad()\n",
    "        self.backward_G()\n",
    "        self.opt_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, epochs, display_every=200):\n",
    "    data = next(iter(val_dl)) # getting a batch for visualizing the model output after fixed intrvals\n",
    "    for e in range(epochs):\n",
    "        loss_meter_dict = create_loss_meters() # function returing a dictionary of objects to \n",
    "        i = 0                                  # log the losses of the complete network\n",
    "        for data in train_dl:\n",
    "            model.setup_input(data) \n",
    "            model.optimize()\n",
    "            update_losses(model, loss_meter_dict, count=data['L'].size(0)) # function updating the log objects\n",
    "            i += 1\n",
    "            if i % display_every == 0:\n",
    "                print(f\"\\nEpoch {e+1}/{epochs}\")\n",
    "                print(f\"Iteration {i}/{len(train_dl)}\")\n",
    "                log_results(loss_meter_dict) # function to print out the losses\n",
    "                visualize(model, data, save=False) # function displaying the model's outputs\n",
    "\n",
    "model = MainModel()\n",
    "train_model(model, train_dl, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test it on your image !\n",
    "\n",
    "image_path = 'images/image.jpg'  # Replace with your test image path\n",
    "image = Image.open(image_path).convert('RGB')\n",
    "from skimage import color\n",
    "# Convert to numpy array and then to Lab color space\n",
    "image_np = np.array(image)\n",
    "image_lab = color.rgb2lab(image_np)\n",
    "\n",
    "# Convert the model to Double\n",
    "model.double()\n",
    "# Extract L, a, b channels\n",
    "L_channel = image_lab[:, :, 0]  # Luminance channel\n",
    "a_channel = image_lab[:, :, 1]  # a color channel\n",
    "b_channel = image_lab[:, :, 2]  # b color channel\n",
    "\n",
    "# Preprocess (example - resize to match model input and convert to tensor)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256, 256)),  # Resize if needed (match your model input size)\n",
    "    # Add any other transformations required by your model\n",
    "])\n",
    "\n",
    "L_tensor = transform(L_channel).unsqueeze(0).double() # Add batch dimension\n",
    "ab_tensor = transform(np.stack([a_channel, b_channel], axis=2)).unsqueeze(0).double()  # Stack a and b, then convert\n",
    "\n",
    "# If you only need L_tensor for inference, you can igno\n",
    "# Assume 'model' is your loaded MainModel instance and 'input_data' is your prepared data\n",
    "input_data = {'L': L_tensor, 'ab': ab_tensor}  # Replace with your actual data\n",
    "model.setup_input(input_data)\n",
    "model.forward()  # No arguments are passed here\n",
    "colorized_image = model.fake_color  # This is your colorized output\n",
    "\n",
    "\n",
    "# Process and visualize the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tensor to a numpy array and change the order of dimensions\n",
    "colorized_image_np = colorized_image.cpu().detach().numpy().squeeze()  # Remove batch dimension if present\n",
    "colorized_image_np = np.transpose(colorized_image_np, (1, 2, 0))  # Change from (C, H, W) to (H, W, C)\n",
    "\n",
    "# Check the range of the output and adjust if necessary\n",
    "# Example: If your model outputs in the range [-1, 1], rescale to the Lab range\n",
    "colorized_image_np[:, :, 0] = (colorized_image_np[:, :, 0] + 1) * 50  # Scale L channel to [0, 100]\n",
    "colorized_image_np[:, :, 1:] = colorized_image_np[:, :, 1:] * 110  # Scale a and b channels\n",
    "\n",
    "# Combine L channel with colorized ab channels\n",
    "complete_lab_image = np.zeros((256, 256, 3), dtype=np.float64)  # Assuming 256x256 output\n",
    "complete_lab_image[:, :, 0] = L_tensor  # L channel from the original image\n",
    "complete_lab_image[:, :, 1:] = colorized_image_np  # colorized ab channels\n",
    "\n",
    "# Convert from Lab to RGB\n",
    "complete_rgb_image = color.lab2rgb(complete_lab_image)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(complete_rgb_image)\n",
    "plt.title(\"Colorized Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "n2a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
